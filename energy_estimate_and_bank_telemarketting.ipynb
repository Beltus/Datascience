{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler #normalize training examples\n",
    "from sklearn.linear_model import Ridge  #linear regression model\n",
    "from sklearn.model_selection import train_test_split #split dataset\n",
    "from sklearn.model_selection import cross_val_score #for scoring model\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RepeatedKFold  #for training and scoring\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error ##evaluate error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset filename for problem 1\n",
    "filename1 = \"ENB2012_data.xlsx\"\n",
    "\n",
    "#load dataset for bank records for problem 2\n",
    "filename = 'bank_additional/bank-additional-full.csv'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def problem1(filename):\n",
    "    \n",
    "    #read in the file as pandas dataframe\n",
    "    dataset = pd.read_excel(filename)\n",
    "\n",
    "    ## Seperate dataset into Input features and output labels\n",
    "\n",
    "    X = dataset.iloc[:, 0:8] # input samples\n",
    "    Y1 = dataset.iloc[:,8] #Y1 labels\n",
    "    Y2 = dataset.iloc[:, 9] #Y2 labels\n",
    "\n",
    "    \n",
    "    #computes the mean and standard deviation to be used for normalization\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    #normalize the input samples\n",
    "    X_normalize = scaler.fit_transform(X)\n",
    "    \n",
    "    #Splitting input samples and Y1 labels\n",
    "    X_train, X_test, Y1_train, Y1_test = train_test_split(X , Y1 , test_size = 0.3, random_state = 42)\n",
    "\n",
    "    #Splitting input samples and Y2 labels\n",
    "    X_train, X_test, Y2_train, Y2_test = train_test_split(X , Y2 , test_size = 0.3, random_state = 42)\n",
    "    \n",
    "    \n",
    "    # =============== TASK 1 =========================\n",
    "    \n",
    "    \n",
    "    #steps for pipeline\n",
    "    steps = [\n",
    "        ('scalar', StandardScaler()),\n",
    "        ('model', Ridge())  # <------ Whatever string you assign here will be used later\n",
    "    ]\n",
    "    \n",
    "    #initialize pipeline\n",
    "    pipeline  = Pipeline(steps = steps)\n",
    "\n",
    "    # Since you have named it as 'model', you need change it to 'model_alpha'\n",
    "    parameters = [ {'model__alpha': (0.001, 0.01, 0.1, 10) } ]\n",
    "  \n",
    "    #list of best alpha for Y1 and Y2\n",
    "    best_alphas = []\n",
    "\n",
    "    #initialize gridsearch with pipeline and parameters\n",
    "    gridSearch = GridSearchCV(estimator = pipeline , param_grid = parameters, \n",
    "                          scoring = None, cv = None)\n",
    "\n",
    "    #loop over the different output variables\n",
    "    for Y_train in [Y1_train, Y2_train]:\n",
    "        #train model\n",
    "        gridSearch.fit(X_train, Y_train)\n",
    "        #print('Best score: %0.3f : '  +  str(gridSearch.best_score_))\n",
    "        #print('Best parameters set:' + str(gridSearch.best_params_))\n",
    "\n",
    "        best_alpha = gridSearch.best_params_\n",
    "        best_score = gridSearch.best_score_\n",
    "\n",
    "        best_alphas.append(best_alpha)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    print(\"Best alpha parameter for Y1:  \", (list(best_alphas[0].values())))\n",
    "    print(\"Best alpha parameter for Y2:  \",  list(best_alphas[1].values()))\n",
    "    \n",
    "    print('==============================*******************-==================\\n')\n",
    "    \n",
    "    \n",
    "    #scores to use for evaluation of model\n",
    "    scorings = ['neg_mean_squared_error' , 'neg_mean_absolute_error']\n",
    "    \n",
    "    #get best alpha\n",
    "    best_alpha = list(best_alphas[0].values())\n",
    "    \n",
    "    #initialize ridge regression model with best alpha\n",
    "    ridge_model = Ridge(alpha = best_alpha[0])\n",
    "    \n",
    "    #initialize repeated kfold\n",
    "    rkf  = RepeatedKFold(n_splits = 10, n_repeats = 10, random_state = None )\n",
    "\n",
    "    #Evaluate Ridge Regression Model with different scores\n",
    "    \n",
    "    scores_dict = dict()\n",
    "    \n",
    "    Y_train = [Y1_train, Y2_train]\n",
    "\n",
    "    ## For loop to compute and print mean and standard deviation for Y1 and Y1 using Ridge Regression with MAE and MSE\n",
    "    for scoring in scorings:\n",
    "\n",
    "        for i in range(len(Y_train)):\n",
    "\n",
    "            score = cross_val_score(ridge_model , X_train, Y_train[i] , scoring = scoring, cv = rkf)\n",
    "            \n",
    "            print('MEAN and STANDARD DEVIATION FOR RIDGE REGRESSION \\n')\n",
    "            print('Mean ' + scoring + ' Y' + str(i+1) + '_Ridge Regression :' + str(np.mean(score)*-1))\n",
    "            print('Standard Deviation_' + 'Y' + str(i+1) + '_Ridge_Regression :' + str(np.std(score)) + '\\n')\n",
    "            \n",
    "    print('============*********************************======= \\n')\n",
    "    \n",
    "    \n",
    "    ### -----Mean and Standard Deviation of the scores for cross validations.-----\n",
    "\n",
    "\n",
    "    \n",
    "    ###================= End of Task 1 ===========================================\n",
    "    \n",
    "    \n",
    "    # =============== TASK 2 Begins =========================\n",
    "\n",
    "    \n",
    "    ##### -- compute Best Parameters For Random Forest Regressor\n",
    "    \n",
    "    #steps for randomforest pipeline\n",
    "    steps = [\n",
    "        ('scalar', StandardScaler()),\n",
    "        ('clf', RandomForestRegressor())  \n",
    "    ]\n",
    "\n",
    "    #initialize repeated kfold\n",
    "    rkf  = RepeatedKFold(n_splits = 10, n_repeats = 10, random_state = None )\n",
    "    \n",
    "    #parameters for Random Forest Regressor\n",
    "    ## parameters to be used by pipeline functions must start with the key name of the function defined in the pipeline object\n",
    "    parameters = [{\n",
    "    'clf__n_estimators' : (10, 50, 100 ,250, 500),\n",
    "    'clf__max_depth': (50, 150, 250),\n",
    "    'clf__min_samples_split': (2,3),\n",
    "    'clf__min_samples_leaf' : (1,2,3),\n",
    "    }]\n",
    "\n",
    "    #initialize pipeline\n",
    "    pipeline  = Pipeline(steps = steps)\n",
    "\n",
    "    #list of best params for Y1 and y2 labels\n",
    "    best_params = []\n",
    "\n",
    "    gridSearch = GridSearchCV(estimator = pipeline , param_grid = parameters, n_jobs = -1 , verbose = 1,\n",
    "                              scoring = None, cv = rkf)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(Y_train)):\n",
    "        \n",
    "        gridSearch.fit(X_train, Y_train[i])\n",
    "        \n",
    "        print('BEST HYPERPARAMETER SCORE AND SET \\n')\n",
    "        print('Best score for Y' + str(i+1) + ' = ' + str(gridSearch.best_score_))\n",
    "        print('Best parameters set:' + str(i+1) + ' = ' + str(gridSearch.best_params_) + '\\n')\n",
    "    \n",
    "    print('============*********************************======= \\n')\n",
    "\n",
    "    ## ---- End of Best Parameter of Random Forest Regressor\n",
    "    \n",
    "    \n",
    "    ##Optimal parameters for Random Forest Regressor\n",
    "\n",
    "    n_estimators = gridSearch.best_params_['clf__n_estimators']\n",
    "    min_samples_leaf = gridSearch.best_params_['clf__min_samples_leaf']\n",
    "    min_samples_split = gridSearch.best_params_['clf__min_samples_split']\n",
    "    max_depth = gridSearch.best_params_['clf__max_depth']\n",
    "    #initialize repeated kfold\n",
    "\n",
    "    rkf  = RepeatedKFold(n_splits = 10, n_repeats = 10, random_state = None )\n",
    "\n",
    "    #n_estimators = 500\n",
    "    #min_samples_leaf = 1\n",
    "    #min_samples_split = 2\n",
    "    #max_depth = 150\n",
    "\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators , min_samples_leaf = min_samples_leaf ,\n",
    "                                  min_samples_split = min_samples_split, max_depth = max_depth)\n",
    "\n",
    "    scorings = ['neg_mean_squared_error' , 'neg_mean_absolute_error']\n",
    "\n",
    "\n",
    "    Y_train = [Y1_train, Y2_train]\n",
    "\n",
    "    ## For loop to compute and print mean and standard deviation for Y1 and Y1 using Ridge Regression with MAE and MSE\n",
    "    for scoring in scorings:\n",
    "\n",
    "        for i in range(len(Y_train)):\n",
    "\n",
    "            score = cross_val_score(model , X_train, Y_train[i] , scoring = scoring, cv = rkf)\n",
    "            \n",
    "            print('MEAN and STANDARD DEVIATION FOR RANDOMFOREST REGRESSOR \\n')\n",
    "            print('Mean ' + scoring + ' Y' + str(i+1) + '_RandomForest :' + str(np.mean(score)*-1))\n",
    "            print('Standard Deviation_' + 'Y' + str(i+1) + '_RandomForest :' + str(np.std(score)) + '\\n')\n",
    "\n",
    "    print('=============****END OF PROBLEM 1****** =======================')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha parameter for Y1:   [0.1]\n",
      "Best alpha parameter for Y2:   [0.1]\n",
      "==============================*******************-==================\n",
      "\n",
      "MEAN and STANDARD DEVIATION FOR RIDGE REGRESSION \n",
      "\n",
      "Mean neg_mean_squared_error Y1_Ridge Regression :8.875120194753686\n",
      "Standard Deviation_Y1_Ridge_Regression :1.9467312100514547\n",
      "\n",
      "MEAN and STANDARD DEVIATION FOR RIDGE REGRESSION \n",
      "\n",
      "Mean neg_mean_squared_error Y2_Ridge Regression :10.671657317086655\n",
      "Standard Deviation_Y2_Ridge_Regression :3.0212245428156836\n",
      "\n",
      "MEAN and STANDARD DEVIATION FOR RIDGE REGRESSION \n",
      "\n",
      "Mean neg_mean_absolute_error Y1_Ridge Regression :2.120951934179041\n",
      "Standard Deviation_Y1_Ridge_Regression :0.27784822287962896\n",
      "\n",
      "MEAN and STANDARD DEVIATION FOR RIDGE REGRESSION \n",
      "\n",
      "Mean neg_mean_absolute_error Y2_Ridge Regression :2.3081317021614907\n",
      "Standard Deviation_Y2_Ridge_Regression :0.3164294235989567\n",
      "\n",
      "============*********************************======= \n",
      "\n",
      "Fitting 100 folds for each of 90 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 380 tasks      | elapsed:   28.9s\n",
      "[Parallel(n_jobs=-1)]: Done 818 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1356 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1970 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2884 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3842 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4944 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6346 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7848 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9000 out of 9000 | elapsed: 19.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST HYPERPARAMETER SCORE AND SET \n",
      "\n",
      "Best score for Y1 = 0.9971294880475459\n",
      "Best parameters set:1 = {'clf__max_depth': 250, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'clf__n_estimators': 500}\n",
      "\n",
      "Fitting 100 folds for each of 90 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 380 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 830 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1368 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2006 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2920 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3882 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4972 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6278 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7488 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9000 out of 9000 | elapsed: 21.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST HYPERPARAMETER SCORE AND SET \n",
      "\n",
      "Best score for Y2 = 0.9643011064129157\n",
      "Best parameters set:2 = {'clf__max_depth': 250, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'clf__n_estimators': 250}\n",
      "\n",
      "============*********************************======= \n",
      "\n",
      "MEAN and STANDARD DEVIATION FOR RANDOMFOREST REGRESSOR \n",
      "\n",
      "Mean neg_mean_squared_error Y1_RandomForest :0.28625681187301294\n",
      "Standard Deviation_Y1_RandomForest :0.1125439835052539\n",
      "\n",
      "MEAN and STANDARD DEVIATION FOR RANDOMFOREST REGRESSOR \n",
      "\n",
      "Mean neg_mean_squared_error Y2_RandomForest :3.016851936060897\n",
      "Standard Deviation_Y2_RandomForest :0.9258509064303088\n",
      "\n",
      "MEAN and STANDARD DEVIATION FOR RANDOMFOREST REGRESSOR \n",
      "\n",
      "Mean neg_mean_absolute_error Y1_RandomForest :0.3472632916841398\n",
      "Standard Deviation_Y1_RandomForest :0.060162236294311014\n",
      "\n",
      "MEAN and STANDARD DEVIATION FOR RANDOMFOREST REGRESSOR \n",
      "\n",
      "Mean neg_mean_absolute_error Y2_RandomForest :1.0637059003494063\n",
      "Standard Deviation_Y2_RandomForest :0.17999586127779793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problem1(filename1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions For Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ******Clean Dataset Function************\n",
    "def data_cleaning(data):\n",
    "    \n",
    "    '''\n",
    "    data : pandas Dataframe of training examples\n",
    "    returns: cleaned_version of the dataset with categorical features replaced by numeric values\n",
    "    '''\n",
    "    \n",
    "    dataset = data\n",
    "    \n",
    "    # list of column names of all categorical features\n",
    "    cat_columns = dataset.select_dtypes(include = ['object']).columns \n",
    "\n",
    "    #convert to numeric data\n",
    "    dataset[cat_columns] = dataset[cat_columns].astype('category').apply(lambda x:x.cat.codes)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "## -******** Plot Graph Function *****************\n",
    "\n",
    "def plot(x, y):\n",
    "    \n",
    "    # plotting the points  \n",
    "    plt.plot(x, y, color = 'green' , marker = 'o') \n",
    "\n",
    "    # naming the x axis \n",
    "    plt.xlabel('C - Parameters') \n",
    "    plt.xscale('log')\n",
    "    # naming the y axis \n",
    "    plt.ylabel('Mean AUC Score') \n",
    "\n",
    "    plt.grid(True)\n",
    "    # giving a title to my graph \n",
    "    plt.title('Logistic Regression Model') \n",
    "\n",
    "    # function to show the plot \n",
    "    plt.show() \n",
    "\n",
    "    \n",
    "    \n",
    "## **********Logistic Regression Classifier Function **************\n",
    "\n",
    "def logistic_regression(X , Y , C):\n",
    "    '''\n",
    "    X : input training split\n",
    "    Y : output labels training split\n",
    "    C : list of C-hyperparameters to be tested during training\n",
    "    returns: dictionary of C-parameter as key and mean AUC as value\n",
    "    '''\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_train = scaler.fit_transform(X)\n",
    "    \n",
    "    Y_train = Y\n",
    "    #initialize repeated kfold\n",
    "    rkf = RepeatedKFold(n_splits = 5 , n_repeats = 5)\n",
    "    \n",
    "    #list of C-hyperp\n",
    "    C_list = C\n",
    "    \n",
    "    #list to hold mean scores\n",
    "    scores = []\n",
    "    #loop through all C values\n",
    "    for c in C_list:\n",
    "        \n",
    "        #logistic regression classifier\n",
    "        model = LogisticRegression(C = c, max_iter = 100)\n",
    "\n",
    "        #training and scoring the model\n",
    "        score = cross_val_score(model , X_train, Y_train , scoring = 'roc_auc', cv = rkf)\n",
    "\n",
    "        #compute the average score for each C-parameter\n",
    "        scores.append(np.mean(score))\n",
    "        \n",
    "    #c-parameter and corresponding mean AUC value\n",
    "    \n",
    "    c_auc_dict = dict(zip(C_list, scores))\n",
    "\n",
    "\n",
    "    return c_auc_dict\n",
    "\n",
    "\n",
    "\n",
    "## **************** RANDOM FOREST CLASSIFIER FUNCTION *********************\n",
    "def random_forest(X , Y):\n",
    "    '''\n",
    "    X : input training split\n",
    "    Y : output labels training split\n",
    "    \n",
    "    returns: gridSearch after fitting with X_train and Y_train\n",
    "    '''\n",
    "    #training input examples  \n",
    "    X_train = X\n",
    "    \n",
    "    #training output labels\n",
    "    Y_train = Y\n",
    "    \n",
    "    #randomforestclassifier object\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    #repeatedKFold object\n",
    "    rkf  = RepeatedKFold(n_splits = 3, n_repeats = 3, random_state = None)\n",
    "\n",
    "    steps = [\n",
    "        ('scaler' , StandardScaler()),\n",
    "        ('clf', model) \n",
    "    ]\n",
    "\n",
    "    #create a pipeline object\n",
    "    pipeline  = Pipeline(steps = steps)\n",
    "    \n",
    "    #parameters for Random Forest classifier\n",
    "    ## parameters to be used by pipeline functions must start with the key name of the function defined in the pipeline object\n",
    "    parameters = [{\n",
    "        'clf__n_estimators' : (10, 50, 100 ,250, 500, 1000),\n",
    "        'clf__max_depth': (50, 150, 250),\n",
    "        'clf__min_samples_split': (2,3),\n",
    "        'clf__min_samples_leaf' : (1,2,3),\n",
    "        }]\n",
    "\n",
    "\n",
    "\n",
    "    #initialize grid search object with pipeline, parameters and cross-validation\n",
    "    gridSearch = GridSearchCV(estimator = pipeline , param_grid = parameters, n_jobs = -1 , verbose = 1,\n",
    "                              scoring = 'roc_auc', cv = rkf)\n",
    "\n",
    "    \n",
    "    #train model and return best parameters\n",
    "    gridSearch.fit(X_train, Y_train)\n",
    "    \n",
    "    return gridSearch\n",
    "    \n",
    "\n",
    "### ************ NEURAL NETWORK MODEL FUNCTION*******************\n",
    "    \n",
    "def neural_network(X, Y):\n",
    "    \n",
    "    '''\n",
    "    X : input training split\n",
    "    Y : output labels training split\n",
    "    \n",
    "    returns: gridSearch after fitting with X_train and Y_train\n",
    "    '''\n",
    "    \n",
    "     #training input examples  \n",
    "    X_train = X\n",
    "    \n",
    "    #training output labels\n",
    "    Y_train = Y\n",
    "    \n",
    "    steps = [('scaler' , StandardScaler()) , ('clf' , MLPClassifier())]\n",
    "\n",
    "    pipeline = Pipeline(steps = steps)\n",
    "\n",
    "    rkf = RepeatedKFold(n_splits = 3 , n_repeats = 3)\n",
    "\n",
    "    parameters = [{\n",
    "                'clf__hidden_layer_sizes' : ((10,10,10), (10,10,10,10), (10,10,10,10,10), (10,10,10,10,10,10)),\n",
    "\n",
    "                'clf__alpha' : (0.00001, 0.0001, 0.001, 0.01, 0.1)   \n",
    "                }]\n",
    "\n",
    "    #'clf__hidden_layers_sizes' : ((10,10,10), (10,10,10,10), (10,10,10,10,10), (10,10,10,10,10,10))\n",
    "\n",
    "    gridSearch = GridSearchCV(pipeline, param_grid = parameters , scoring = 'roc_auc' , n_jobs = -1, cv = rkf, verbose = 1)\n",
    "\n",
    "    #fitting training data\n",
    "    gridSearch.fit(X_train, Y_train)\n",
    "    \n",
    "    return gridSearch\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem2(filename):\n",
    "    \n",
    "    data = pd.read_csv(filename , sep = ';' , index_col = False)\n",
    "\n",
    "    #data.head()\n",
    "    #statistical information\n",
    "\n",
    "\n",
    "    #data.describe()\n",
    "\n",
    "    ##convert categorical features to numeric\n",
    "\n",
    "    #replace categorical features with numeric features\n",
    "    dataset = data_cleaning(data)\n",
    "\n",
    "    #dataset.head()\n",
    "    #Extract the imput and \n",
    "    X = dataset.iloc[:, 0:-1]\n",
    "    Y  = dataset.iloc[: , -1]\n",
    "\n",
    "    #split dataset to train and test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y , test_size = 0.3)\n",
    "    #dataset.head()\n",
    "\n",
    "    \n",
    "    ##=======   TASK 1: LOGISTIC REGRESSION ============================\n",
    "    \n",
    "    #c-hyperparameters for training logistic regression model\n",
    "    C_params = np.array([0.0001,0.0005,0.001,0.005, 0.01, 0.05 ,0.1, 0.5, 1.0, 5, 10.0, 50, 100.0, 250, 500, 1000.0, 2500, 5000, 7500, 10000.0])\n",
    "\n",
    "    #get dictionary of c-parameter and mean auc of logistic regression model\n",
    "    c_mean_auc_dict = logistic_regression( X_train, Y_train , C_params)\n",
    "\n",
    "    best_c = max( c_mean_auc_dict, key=lambda key:  c_mean_auc_dict[key])\n",
    "\n",
    "    print('C parameter with the Highest AUC score : ' + str(best_c) )\n",
    "    print('\\n')\n",
    "\n",
    "    ## plot graph of mean AUC scores with C-parameters\n",
    "    plot(list(c_mean_auc_dict.keys()) , list(c_mean_auc_dict.values()))\n",
    "\n",
    "      \n",
    "    ##=======   TASK 2: RANDOM FOREST MODEL ============================\n",
    "\n",
    "    #train randomforest model using gridsearch return the trained gridsearch\n",
    "    gridSearch_RandomForest = random_forest(X_train, Y_train)\n",
    "   \n",
    "    print('===== Random Forest Training Results =======')\n",
    "    print('Best Score For RandomForest Classifier :' + str(gridSearch_RandomForest.best_score_))\n",
    "    print('Best Parameters set:' + str(gridSearch_RandomForest.best_params_))\n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "    ##=======   TASK 3: NEURAL NETWORK MODEL ============================\n",
    "\n",
    "    #train neural network using gridsearch and different parameters.\n",
    "    gridSearch_NeuralNetwork = neural_network(X_train, Y_train)\n",
    "    \n",
    "    \n",
    "    print('===== Neural Network Training Results =======')\n",
    "    print('Best Score For Neural Network :' + str(gridSearch_NeuralNetwork.best_score_))\n",
    "    print('Best Parameters set:' + str(gridSearch_NeuralNetwork.best_params_))\n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "    ##=======   TASK 4 ============================\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_norm = scaler.fit_transform(X)\n",
    "    \n",
    "    #initialize 5 fold cross validation with randomly split data\n",
    "    kf = KFold(n_splits = 5 , shuffle = True)\n",
    "    \n",
    "    #initialize logistic regression classifier\n",
    "    logistic_model = LogisticRegression() #default C value is 1\n",
    "    \n",
    "    #average prediction with logistic model using 5-fold CV\n",
    "    lr_predict = cross_val_predict(logistic_model, X_norm, Y, cv = kf )\n",
    "    \n",
    "    #classification report\n",
    "    print('Classification Report For Logistic Regression')\n",
    "    print(classification_report(Y, lr_predict))\n",
    "    print('\\n')\n",
    "    \n",
    "    #******Prediction with Random Forest Classifier *****\n",
    "    \n",
    "    #best parameters for random forest classifier\n",
    "    best_params_RF = gridSearch_RandomForest.best_params_\n",
    "\n",
    "    n_estimators = best_params_RF['clf__n_estimators']\n",
    "    min_samples_leaf = best_params_RF['clf__min_samples_leaf']\n",
    "    min_samples_split = best_params_RF['clf__min_samples_split']\n",
    "    max_depth  = best_params_RF['clf__max_depth']\n",
    "    \n",
    "    ##initialize classifier object\n",
    "    randomForest_model = RandomForestClassifier(n_estimators = n_estimators , max_depth =  max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf)\n",
    "    \n",
    "    ##predict output with random forest model using 5 fold cv\n",
    "    rf_predict = cross_val_predict(randomForest_model , X_norm, Y, cv = kf)\n",
    "    \n",
    "    print('Classification Report For Random Forest')\n",
    "    print(classification_report(Y, rf_predict))\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    # ******* Prediction with Neural Network Model ********\n",
    "    \n",
    "    best_params_NN = gridSearch_NeuralNetwork.best_params_\n",
    "    \n",
    "    alpha = best_params_NN['clf__alpha']\n",
    "    hidden_layer_sizes = best_params_NN['clf__hidden_layer_sizes']\n",
    "    \n",
    "    nn_model = MLPClassifier(hidden_layer_sizes = hidden_layer_sizes , alpha = alpha)\n",
    "    \n",
    "    ##predict output with random forest model using 5 fold cv\n",
    "    nn_predict = cross_val_predict(nn_model , X_norm, Y, cv = kf)\n",
    "    \n",
    "    print('Classification Report For Neural Network')\n",
    "    print(classification_report(Y, nn_predict))\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C parameter with the Highest AUC score : 1000.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 9 folds for each of 108 candidates, totalling 972 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 27.9min\n",
      "[Parallel(n_jobs=-1)]: Done 972 out of 972 | elapsed: 35.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Random Forest Training Results =======\n",
      "Best Score For RandomForest Classifier :0.9471735876342859\n",
      "Best Parameters set:{'clf__max_depth': 150, 'clf__min_samples_leaf': 3, 'clf__min_samples_split': 2, 'clf__n_estimators': 1000}\n",
      "\n",
      "\n",
      "Fitting 9 folds for each of 20 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 17.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Neural Network Training Results =======\n",
      "Best Score For Neural Network :0.9400006888807617\n",
      "Best Parameters set:{'clf__alpha': 0.1, 'clf__hidden_layer_sizes': (10, 10, 10)}\n",
      "\n",
      "\n",
      "Classification Report For Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95     36548\n",
      "           1       0.67      0.41      0.51      4640\n",
      "\n",
      "    accuracy                           0.91     41188\n",
      "   macro avg       0.80      0.69      0.73     41188\n",
      "weighted avg       0.90      0.91      0.90     41188\n",
      "\n",
      "\n",
      "\n",
      "Classification Report For Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95     36548\n",
      "           1       0.66      0.51      0.58      4640\n",
      "\n",
      "    accuracy                           0.92     41188\n",
      "   macro avg       0.80      0.74      0.76     41188\n",
      "weighted avg       0.91      0.92      0.91     41188\n",
      "\n",
      "\n",
      "\n",
      "Classification Report For Neural Network\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95     36548\n",
      "           1       0.64      0.54      0.59      4640\n",
      "\n",
      "    accuracy                           0.91     41188\n",
      "   macro avg       0.79      0.75      0.77     41188\n",
      "weighted avg       0.91      0.91      0.91     41188\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problem2(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
